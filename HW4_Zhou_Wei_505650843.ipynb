{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "HW4_Zhou_Wei_505650843.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCnyXXpJlsvn"
      },
      "source": [
        "#  Homework 4 for CS 247 : Advanced Data Mining Learning\n",
        "\n",
        "\n",
        "### Due: 11:59 pm 04/29\n",
        "\n",
        "##### Please read the Homework Guidance (uploaded to CCLE) carefully and make sure you fulfill all the requirements.\n",
        "\n",
        "\n",
        "\n",
        "__Name__: [Wei Zhou]\n",
        "\n",
        "__UID__: [505650843]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49Iq3a7lsvx"
      },
      "source": [
        "## Problem 1: Multinomial Mixture Model (30 pts)\n",
        "\n",
        "Similar to Gaussian Mixture Model, Multinomial Mixture Model can also be used for soft clustering. \n",
        "For example, in the case of document clustering, each cluster $z$ is a multinomial distribution with parameters $\\beta_k$, and $x_i|z_i \\sim mutlinomial(\\beta_k)$, where $x_i$ is a bag-of-words representation vector for document $i$. Please write down the EM algorithm for soft document clustering under Multinomial Mixture Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcUYlQCmlsvy"
      },
      "source": [
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmDhePW2lsvy"
      },
      "source": [
        "[Your Answer]\n",
        "E step: https://drive.google.com/file/d/13_5tKOKJehiu-cNPm4OsNZ5RAebW5NIK/view?usp=sharing \n",
        "\n",
        "M step: https://drive.google.com/file/d/1Z6CeZFSVLG3muH6kNxgjGxtpFhGarxWe/view?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp1CXP9Vlsvz"
      },
      "source": [
        "## Problem 2: Probabilistic Latent Semantic Analysis （70 pts)\n",
        "\n",
        "In this problem, you are going to implement the pLSA algorithm on the dataset, and answer some questions about pLSA Initialization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie5W0v8Ulsvz"
      },
      "source": [
        "### Part 1: pLSA Implementation (45pts = 15 + 15 + 15)\n",
        "\n",
        "In this part, you are going to implement the pLSA algorithm. We provide 2 datasets for you. Dataset1 is very simple and it's mainly for you to check whether your implemenation is correct or not. Dataset2 is more complicated. **Please report your results with dataset2**. Please note that, for dataset1, you should set num_topics=4, and for dataset2, you should set num_topics=10.\n",
        "\n",
        "Hint: You can check your implemenation with dataset 1. If your implementation is correct, you should be able to get the following results (please do not change any of our parameter setting and random seeds):\n",
        "\n",
        "['dragon', 'ball', 'kakarot', 'world', 'action']\n",
        "\n",
        "['dragon', 'ball', 'kakarot', 'anim', 'seri']\n",
        "\n",
        "['covid', 'lab', 'coronaviru', 'california', 'lo']\n",
        "\n",
        "['covid', 'wuhan', 'person', 'spy', 'agenc']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNItMDfaq41Z",
        "outputId": "ca254bf1-3a65-464a-e17b-0f912c547cd3"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIHAf70Blsv0"
      },
      "source": [
        "# import neccessary libraries\n",
        "\n",
        "import numpy as np # please update your numpy version if you get the error \"module 'numpy.random' has no attribute 'default_rng'\"\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from collections import defaultdict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD54FPOdlsv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1606f2-6d9f-46ab-f764-4a8b7d3702d5"
      },
      "source": [
        "# load and pre-process data\n",
        "# word2id : a map mapping terms to their corresponding ids\n",
        "# id2word : a map mapping ids to terms\n",
        "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
        "\n",
        "\n",
        "wordCounts = []\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "\n",
        "datasetFilePath = '/gdrive/MyDrive/Colab Notebooks/CS247/hw4/dataset2.txt' \n",
        "# you can use dataset1 to check your implementation, but you should use dataset2 to report your results#\n",
        "num_topics = 10\n",
        "# for dataset1, please use num_topics=4; for dataset2, please use num_topics=10#\n",
        "\n",
        "fin = open(datasetFilePath,encoding='UTF-8')\n",
        "documents = fin.readlines()\n",
        "for doc in documents:\n",
        "    words = preprocess_string(doc)\n",
        "    word_count = defaultdict(lambda: 0)\n",
        "    for word in words:\n",
        "        if word not in word2id:\n",
        "            idx = len(id2word)\n",
        "            word2id[word] = idx\n",
        "            id2word[idx]  = word\n",
        "        word_count[word] += 1\n",
        "    wordCounts += [word_count]\n",
        "    \n",
        "    \n",
        "num_doc = len(documents)\n",
        "num_words = len(word2id)  \n",
        "\n",
        "# generate the document-word matrix\n",
        "X = np.zeros([num_doc, num_words], np.int)\n",
        "\n",
        "for word in word2id:\n",
        "    j = word2id[word]\n",
        "    for i in range(0, num_doc):\n",
        "        if word in wordCounts[i]:\n",
        "            X[i, j] = wordCounts[i][word]\n",
        "            \n",
        "\n",
        "print(\"Number of Documents: \", num_doc)\n",
        "print(\"Number of Words in the Vocabulary: \", num_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Documents:  16\n",
            "Number of Words in the Vocabulary:  728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNMPM1AHlsv1"
      },
      "source": [
        "# pLSA class\n",
        "\n",
        "class PLSA():\n",
        "    def __init__(self, num_doc, num_words, num_topics):\n",
        "        \n",
        "        self.num_doc = num_doc\n",
        "        self.num_words = num_words\n",
        "        self.num_topics = num_topics\n",
        "        \n",
        "        # theta_dz: topic distribution for each document p(z|d)\n",
        "        np.random.seed(0)\n",
        "        self.theta = np.random.random([num_doc, num_topics])\n",
        "\n",
        "        # beta_zw:  word distribution for each topic p(w|z) \n",
        "        np.random.seed(1)\n",
        "        self.beta = np.random.random([num_topics, num_words])\n",
        "\n",
        "        # p[i, j, k] : lower bound of p(zk|di,wj)\n",
        "        self.p = np.zeros([num_doc, num_words, num_topics])\n",
        "\n",
        "        self.theta /= np.sum(self.theta, axis=1).reshape(-1,1)\n",
        "        self.beta  /= np.sum(self.beta, axis=1).reshape(-1,1)\n",
        "\n",
        "    def EStep(self):\n",
        "        '''\n",
        "        TODO:\n",
        "            update lower bound p\n",
        "        '''\n",
        "        for i in range(0, num_doc):\n",
        "          for j in range(0, num_words):\n",
        "              denominator = 0;\n",
        "              for k in range(0, num_topics):\n",
        "                  self.p[i, j, k] = self.beta[k, j] * self.theta[i, k];\n",
        "                  denominator += self.p[i, j, k];\n",
        "              if denominator == 0:\n",
        "                  for k in range(0, num_topics):\n",
        "                      self.p[i, j, k] = 0;\n",
        "              else:\n",
        "                  for k in range(0, num_topics):\n",
        "                      self.p[i, j, k] /= (denominator + 0.0000000001) \n",
        "                        \n",
        "    def MStep(self, X):\n",
        "        '''\n",
        "        TODO:\n",
        "            update beta\n",
        "        '''\n",
        "        for k in range(0, num_topics):\n",
        "          denominator = 0\n",
        "          for j in range(0, num_words):\n",
        "              self.beta[k, j] = 0\n",
        "              for i in range(0, num_doc):\n",
        "                  self.beta[k, j] += X[i, j] * self.p[i, j, k]\n",
        "              denominator += self.beta[k, j]\n",
        "          if denominator == 0:\n",
        "              for j in range(0, num_words):\n",
        "                  self.beta[k, j] = 1.0 / num_words\n",
        "          else:\n",
        "              for j in range(0, num_words):\n",
        "                  self.beta[k, j] /= (denominator + 0.0000000001)   \n",
        "        \n",
        "      \n",
        "        \n",
        "        '''\n",
        "        TODO:\n",
        "            update theta\n",
        "        '''\n",
        "        \n",
        "        for i in range(0, num_doc):\n",
        "            for k in range(0, num_topics):\n",
        "                self.theta[i, k] = 0\n",
        "                denominator = 0\n",
        "                for j in range(0, num_words):\n",
        "                    self.theta[i, k] += X[i, j] * self.p[i, j, k]\n",
        "                    denominator += X[i, j];\n",
        "                if denominator == 0:\n",
        "                    self.theta[i, k] = 1.0 / num_topics\n",
        "                else:\n",
        "                    self.theta[i, k] /= (denominator +0.0000000001 )\n",
        "                    \n",
        "    # calculate the log likelihood\n",
        "    def LogLikelihood(self, X):\n",
        "        loglikelihood = 0\n",
        "        for i in range(0, self.num_doc):\n",
        "            for j in range(0, self.num_words):\n",
        "                tmp = 0\n",
        "                for k in range(0, self.num_topics):\n",
        "                    tmp += self.beta[k, j] * self.theta[i, k]\n",
        "                if tmp > 0:\n",
        "                    loglikelihood += X[i, j] * np.log(tmp)\n",
        "        return loglikelihood\n",
        "    \n",
        "    "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Jml7SKXHlsv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67681d7f-f1d7-462f-d971-e2ad902482c0"
      },
      "source": [
        "# test the PLSA class\n",
        "\n",
        "plsa = PLSA(num_doc, num_words, num_topics)\n",
        "\n",
        "# EM algorithm\n",
        "for i in range(0, 30):\n",
        "    plsa.EStep()\n",
        "    plsa.MStep(X)\n",
        "    log_likelihood = plsa.LogLikelihood(X)\n",
        "    print(\"iteration %d, LogLikelihood: %.3f\" % (i, log_likelihood))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0, LogLikelihood: -8593.509\n",
            "iteration 1, LogLikelihood: -8370.475\n",
            "iteration 2, LogLikelihood: -8087.982\n",
            "iteration 3, LogLikelihood: -7767.412\n",
            "iteration 4, LogLikelihood: -7425.230\n",
            "iteration 5, LogLikelihood: -7132.968\n",
            "iteration 6, LogLikelihood: -6938.684\n",
            "iteration 7, LogLikelihood: -6819.566\n",
            "iteration 8, LogLikelihood: -6737.579\n",
            "iteration 9, LogLikelihood: -6672.181\n",
            "iteration 10, LogLikelihood: -6616.028\n",
            "iteration 11, LogLikelihood: -6569.560\n",
            "iteration 12, LogLikelihood: -6529.477\n",
            "iteration 13, LogLikelihood: -6490.083\n",
            "iteration 14, LogLikelihood: -6458.853\n",
            "iteration 15, LogLikelihood: -6445.527\n",
            "iteration 16, LogLikelihood: -6438.536\n",
            "iteration 17, LogLikelihood: -6434.878\n",
            "iteration 18, LogLikelihood: -6432.785\n",
            "iteration 19, LogLikelihood: -6430.464\n",
            "iteration 20, LogLikelihood: -6428.539\n",
            "iteration 21, LogLikelihood: -6427.353\n",
            "iteration 22, LogLikelihood: -6426.471\n",
            "iteration 23, LogLikelihood: -6425.848\n",
            "iteration 24, LogLikelihood: -6425.541\n",
            "iteration 25, LogLikelihood: -6425.439\n",
            "iteration 26, LogLikelihood: -6425.407\n",
            "iteration 27, LogLikelihood: -6425.391\n",
            "iteration 28, LogLikelihood: -6425.378\n",
            "iteration 29, LogLikelihood: -6425.366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7xbS7yIlsv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a332a7-3f97-491f-9556-3e1afa0ea061"
      },
      "source": [
        "# print the top-5 frequent words in each topic\n",
        "\n",
        "for i in plsa.beta:\n",
        "    topic_word = []\n",
        "    for idx in (-i).argsort()[:5]:\n",
        "        topic_word += [id2word[idx]]\n",
        "    print(topic_word)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['luffi', 'dressrosa', 'pirat', 'ac', 'flame']\n",
            "['zou', 'pirat', 'luffi', 'big', 'mom']\n",
            "['haki', 'color', 'allow', 'snail', 'den']\n",
            "['luffi', 'pirat', 'captain', 'crew', 'name']\n",
            "['crew', 'pirat', 'govern', 'franki', 'hat']\n",
            "['island', 'pose', 'field', 'log', 'magnet']\n",
            "['fruit', 'devil', 'user', 'power', 'sea']\n",
            "['manga', 'seri', 'anim', 'licens', 'piec']\n",
            "['line', 'sea', 'grand', 'world', 'blue']\n",
            "['pirat', 'baroqu', 'alabasta', 'work', 'line']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4lioT9lsv3"
      },
      "source": [
        "### Part 2: pLSA Initialization (25 pts = 10 + 15)\n",
        "\n",
        "Please answer the following two questions about pLSA initialization:\n",
        "\n",
        "1. Let K denote the number of topics, N denote the total number of words in the dictionary. Is it a good initialization to set θd’s and βk’s as uniform distribution, i.e., θdk = 1/K for every d and k, and  βkw = 1/N for every k and w, where N is the total number of words in the dictionary? Why (please give some formulas to illustrate your conclusion)? \n",
        "\n",
        "2. Can you give an example of bad initialization? (if the initialization in question 1 is a bad initialization, you **CANNOT** use it as answer for question 2) Why this is a bad initialization (please give some formulas to illustrate your conclusion)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRORtXbxlsv4"
      },
      "source": [
        "#### Write Your answer here:\n",
        "\n",
        "https://drive.google.com/file/d/1V7D60ZxYxhuFrOty7d3Df4BEukeVYrnN/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdMBVbaaz-Mh"
      },
      "source": [
        "# Citation\n",
        "\n",
        "PLSA:\n",
        "\n",
        "https://github.com/laserwave \n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
        "https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41\n",
        "http://zhikaizhang.cn/2016/06/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8BPLSA/\n",
        "\n",
        "\n",
        "Collaboration & help: Study group: Jessica Bojorquez, John Fritsche Lucius Wang, Office Hour content (Thank you Yewen! ) \n",
        "\n",
        "\n",
        "Initialization: \n",
        "https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e\n",
        "\n"
      ]
    }
  ]
}