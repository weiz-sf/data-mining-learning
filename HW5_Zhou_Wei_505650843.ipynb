{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_Zhou_Wei_505650843.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbCTBQVYFGUr"
      },
      "source": [
        "#  Homework 5 for CS 247 : Advanced Data Mining Learning\n",
        "\n",
        "\n",
        "### Due: 11:59 pm 05/06\n",
        "\n",
        "##### Please read the Homework Guidance (uploaded to CCLE) carefully and make sure you fulfill all the requirements.\n",
        "\n",
        "\n",
        "\n",
        "__Name__: [Wei Zhou]\n",
        "\n",
        "__UID__: [505650843]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl1NVCBzFGUs"
      },
      "source": [
        "## Problem 1: Skip-Gram (50 pts = 40 + 10)\n",
        "\n",
        "In this problem, you are goint to implement skip-gram model with negative sampling in Pytorch, apply it on the 20-newsgroup dataset, and compare your SkipGram implementation with the gensim implementation by looking at top-10 most similar words with \"pittsburgh\". Please note that, your SkipGram and gensim skipgram don't have to have exactly same results.\n",
        "\n",
        "Hint:\n",
        "* Running time would be long, please start early and be patient. You can reduce the number of iteration **itr_num** to 1 when you are debugging, but make sure to use **itr_num=20** to report your results.\n",
        "* You may find this tutorial for the gensim library helpful if you want to get familiar with gensim: https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here\n",
        "\n",
        "Suggestions:\n",
        "* Please think about which parameters you need to define.\n",
        "* Please make sure you know what shape each operation expects. Use .view() if you need to\n",
        "  reshape.\n",
        "  \n",
        "Possible ERROR message for the code skeleton (The code skeleton is bug-free, this ERROR message is only caused by setting issues):\n",
        "* You may get error message: **ValueError: unable to read local cache '/Users/emilywang/gensim-data/information.json' during fallback, connect to the Internet and retry**. Here, \"Users/emilywang/\" should be your own path for gesim-data.\n",
        "* This indicates the gensim-data folder on your device does not include the information.json file.\n",
        "* To solve this problem, you should put the provided information.json file (in our homework zip file) under the indicated path.\n",
        "* For MAC users, you may see this error afterwards: **<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)>.** You can follow: https://timonweb.com/python/fixing-certificate_verify_failed-error-when-trying-requests-html-out-on-mac/ to solve this problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQhNEw53FGUt"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imad6cecFGUw"
      },
      "source": [
        "# load dataset\n",
        "\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "import gensim.downloader as api\n",
        "\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "sent_ids  = []\n",
        "sent_wds  = []\n",
        "\n",
        "word_count = {}\n",
        "\n",
        "itr_num = 20\n",
        "\n",
        "dataset = api.load(\"20-newsgroups\")  # load dataset as iterable\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEIh9Zm5FGU4",
        "outputId": "60c04f8c-bb5d-4bad-c483-249f3bd38df1"
      },
      "source": [
        "# data processing\n",
        "\n",
        "for data in dataset:\n",
        "    doc = data['data']\n",
        "    words = preprocess_string(doc)\n",
        "    for word in words:\n",
        "        if word not in word_count:\n",
        "            word_count[word] = 0\n",
        "        word_count[word] += 1\n",
        "\n",
        "MIN_COUNT = 5    # Only consider words whose frequency is larger than MIN_COUNT\n",
        "WINDOW_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "for data in dataset:\n",
        "    doc = data['data']\n",
        "    words = preprocess_string(doc)\n",
        "    sent_id = []\n",
        "    sent_wd = []\n",
        "    for word in words:\n",
        "        if word_count[word] < MIN_COUNT:\n",
        "            continue\n",
        "        if word not in word2id:\n",
        "            idx = len(id2word)\n",
        "            word2id[word] = idx\n",
        "            id2word[idx]  = word\n",
        "        sent_id += [word2id[word]]\n",
        "        sent_wd += [word]\n",
        "    if len(sent_wd) <= WINDOW_SIZE * 2:\n",
        "        continue\n",
        "    sent_ids += [sent_id]\n",
        "    sent_wds += [sent_wd]\n",
        "    \n",
        "data = []\n",
        "for sent in sent_ids:\n",
        "    for i in range(WINDOW_SIZE, len(sent) - WINDOW_SIZE):\n",
        "        context = [sent[i - WINDOW_SIZE: i] + sent[i+1: i + WINDOW_SIZE + 1]]\n",
        "        target  = sent[i]\n",
        "        data.append((context, target))\n",
        "print(\"data_length:\",len(data))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_length: 2472017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXasop1vWhr1"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, hidden_size = 100):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.u_emb = nn.Embedding(vocab_size, hidden_size) #output\n",
        "        self.v_emb = nn.Embedding(vocab_size, hidden_size) #input\n",
        "\n",
        "    def forward(self, idx):\n",
        "        return self.u_emb(idx)\n",
        "    def loss(self, pos_data, neg_data):\n",
        "        '''\n",
        "            TODO: \n",
        "                Fill in this blank: Train the word embedding based on Skip-gram algorithm\n",
        "        '''\n",
        "#TODO   \n",
        "\n",
        "        pos_target = []\n",
        "        pos_context = []\n",
        "        for context, target in pos_data:\n",
        "          pos_context.append(context[0]) \n",
        "          pos_target.append([target] * len(context[0]))\n",
        "        pos_context = np.array(pos_context).flatten() \n",
        "        pos_target = np.array(pos_target).flatten()\n",
        "\n",
        "        target = Variable(torch.LongTensor(pos_target))\n",
        "        context = Variable(torch.LongTensor(pos_context))\n",
        "        v = self.v_emb(target)\n",
        "        u = self.u_emb(context)\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "        positive_val = self.log_sigmoid(torch.sum(u * v, dim = 1)).squeeze()\n",
        "        \n",
        "        neg_words = Variable(torch.LongTensor(neg_data))\n",
        "        u_hat = self.u_emb(neg_words)\n",
        "        neg_vals = torch.bmm(u_hat, v.unsqueeze(2)).squeeze()\n",
        "\n",
        "        neg_val = self.log_sigmoid(-torch.sum(neg_vals, dim=1)).squeeze()\n",
        "\n",
        "        loss = positive_val + neg_val\n",
        "        return -loss.mean()/batch_size\n",
        "#TODO "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqsq6gdkWhr2",
        "outputId": "a5598805-702f-40d2-bc52-fd529e0562f8"
      },
      "source": [
        "# apply the SkipGram model to the 20-newsgroup dataset\n",
        "\n",
        "skipgram = SkipGram(len(word2id))\n",
        "optimizer = optim.Adam(skipgram.parameters())\n",
        "\n",
        "vocabulary = {key: value for key, value in word_count.items() if value >= MIN_COUNT}\n",
        "N =sum(vocabulary.values())\n",
        "word_prob = {key: value/N for (key, value) in vocabulary.items()}\n",
        "word_ID = list(word_prob.keys())\n",
        "word_ID = [word2id[word] for word in word_ID]\n",
        "neg_sample_count = 10\n",
        "\n",
        "batch_size = 1280\n",
        "l = []\n",
        "for i in range(itr_num):\n",
        "    print(\"iteration: \",i)\n",
        "    s = 0\n",
        "    for bid in range(len(data) // batch_size):\n",
        "        #print(\"iteration: \",i, \", batch: \", bid) # there are around 2000 batches per iteration, you may want to print the batch number to check the curret progress\n",
        "        optimizer.zero_grad()\n",
        "        positive_data = data[bid * batch_size : (bid + 1) * batch_size]\n",
        "        '''\n",
        "            TODO: \n",
        "                Conduct negative sample for negative words, based on word frequency\n",
        "        '''\n",
        "        #TODO\n",
        "        neg_data =  np.random.choice(word_ID,size = (4*batch_size, neg_sample_count), p=list(word_prob.values()))\n",
        "        #TODO \n",
        "        loss = skipgram.loss(positive_data, neg_data)\n",
        "        loss.backward()\n",
        "        s += loss\n",
        "        optimizer.step()\n",
        "    l.append(s.item()/(len(data) // batch_size))\n",
        "    print(\"Average Loss for the current iteration: \", l[i])\n",
        "    print(\"-----------------------------------\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration:  0\n",
            "Average Loss for the current iteration:  0.008969297562159138\n",
            "-----------------------------------\n",
            "iteration:  1\n",
            "Average Loss for the current iteration:  0.004436519589219175\n",
            "-----------------------------------\n",
            "iteration:  2\n",
            "Average Loss for the current iteration:  0.0029863933403216636\n",
            "-----------------------------------\n",
            "iteration:  3\n",
            "Average Loss for the current iteration:  0.0021721396898124455\n",
            "-----------------------------------\n",
            "iteration:  4\n",
            "Average Loss for the current iteration:  0.0016685471764381287\n",
            "-----------------------------------\n",
            "iteration:  5\n",
            "Average Loss for the current iteration:  0.0013444457012158734\n",
            "-----------------------------------\n",
            "iteration:  6\n",
            "Average Loss for the current iteration:  0.001130958223268814\n",
            "-----------------------------------\n",
            "iteration:  7\n",
            "Average Loss for the current iteration:  0.0009844022239199085\n",
            "-----------------------------------\n",
            "iteration:  8\n",
            "Average Loss for the current iteration:  0.0008811657444792421\n",
            "-----------------------------------\n",
            "iteration:  9\n",
            "Average Loss for the current iteration:  0.0008060601879931805\n",
            "-----------------------------------\n",
            "iteration:  10\n",
            "Average Loss for the current iteration:  0.0007487936133488912\n",
            "-----------------------------------\n",
            "iteration:  11\n",
            "Average Loss for the current iteration:  0.0007046320350722042\n",
            "-----------------------------------\n",
            "iteration:  12\n",
            "Average Loss for the current iteration:  0.0006696492016840203\n",
            "-----------------------------------\n",
            "iteration:  13\n",
            "Average Loss for the current iteration:  0.0006413699924976698\n",
            "-----------------------------------\n",
            "iteration:  14\n",
            "Average Loss for the current iteration:  0.0006184805124797455\n",
            "-----------------------------------\n",
            "iteration:  15\n",
            "Average Loss for the current iteration:  0.0005987305396705758\n",
            "-----------------------------------\n",
            "iteration:  16\n",
            "Average Loss for the current iteration:  0.00058195481085518\n",
            "-----------------------------------\n",
            "iteration:  17\n",
            "Average Loss for the current iteration:  0.0005676493874366637\n",
            "-----------------------------------\n",
            "iteration:  18\n",
            "Average Loss for the current iteration:  0.0005554818909505314\n",
            "-----------------------------------\n",
            "iteration:  19\n",
            "Average Loss for the current iteration:  0.0005444603716999925\n",
            "-----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPej4SCFGVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097d533e-ed7c-4a90-b32e-1e25384cccd5"
      },
      "source": [
        "# Compare the results with standard gensim implementation\n",
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "# model = Word2Vec(sent_wds, min_count=1, window=2, size = 100, workers = 4)\n",
        "model = Word2Vec(sent_wds)\n",
        "print(model.wv.most_similar('pittsburgh'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('boston', 0.8298497796058655), ('minnesota', 0.8298306465148926), ('montreal', 0.8094165921211243), ('chicago', 0.8062117695808411), ('milwauke', 0.801995575428009), ('baltimor', 0.7846183776855469), ('cincinnati', 0.7752628922462463), ('alberta', 0.7724369168281555), ('andov', 0.7466251254081726), ('carnegi', 0.7450976371765137)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjALDCnVFGVD"
      },
      "source": [
        "## Problem 2: PageRank (50 pts)\n",
        "\n",
        "In this problem, you are going to do some proofs for the PageRank algorithm, then implement it and apply the implemented model on a citation dataset. Finally, you are going to extend it to personalized PageRank.\n",
        "\n",
        "Please download the citation dataset from https://aminer.org/dblp_citation (Version 1). In the page, you will be able to see a very detailed README regarding the organization of the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKD_0fX0aFSD"
      },
      "source": [
        "### Part 1: PageRank Score Without Teleport (10pts)\n",
        "\n",
        "Prove that, for a connected undirected graph, where the adjacency matrix $A = A^T$ , the PageRank score (without teleport) for node i is proportional to its degree $d_i$, i.e., $r_i = d_i/2|E|$, where |E| is the total number of edges in the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1JO3iIgWhr5"
      },
      "source": [
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufOoftZBWhr5"
      },
      "source": [
        "[Your Answer] https://drive.google.com/file/d/1fh1Q3fV3aqmcAs8mFhtg6KI_44rStOt9/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujTzXL0CWhr5"
      },
      "source": [
        "### Part 2: PageRank Score With Teleport (10pts)\n",
        "\n",
        "Prove that, the closed form solution to PageRank with teleport is: \n",
        "\n",
        "$r = (1 − \\beta)(I − \\beta M)^{-1}  \\mathbb{1}/N$\n",
        "\n",
        "\n",
        "where 1 − β is the teleport probability, $M = (D^{-1}A)^T$ , $\\mathbb{1}$ is the all one vector with dimentionality N, and N is the total number of nodes in the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9C-mfRbWhr5"
      },
      "source": [
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTTrfzQ0Whr6"
      },
      "source": [
        "[Your Answer]  https://drive.google.com/file/d/1fh1Q3fV3aqmcAs8mFhtg6KI_44rStOt9/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghxcRcnmWhr6"
      },
      "source": [
        "### Part 3: Implement PageRank With Teleport (10 pts)\n",
        "\n",
        "Implement PageRank with teleport on Conference citation network. Show the top 50 conferences according to their PageRank scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x1L3qPOWhr6"
      },
      "source": [
        "# import libraries\n",
        "\n",
        "from scipy.sparse import coo_matrix\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg import inv\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvXHP3whl0ks",
        "outputId": "08a2f1dc-b2c6-42f8-80ec-6b228953e0b3"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HGfQbELWhr6"
      },
      "source": [
        "def preprocessing():\n",
        "#     parse the raw dataset, \n",
        "#     extract useful information, \n",
        "#     return the parsed entities.\n",
        "\n",
        "    with open('/gdrive/MyDrive/Colab Notebooks/CS247/hw5/DBLPOnlyCitationOct19.txt') as file:\n",
        "        id_pub, id_cite = {}, {}\n",
        "        _cite_temp = []\n",
        "        pub_list = {}\n",
        "        for line in file:\n",
        "            if not line.find('#c'):\n",
        "                pub = line[2:-1]\n",
        "                if pub == '':\n",
        "                    pub = 'noname'\n",
        "                if pub not in pub_list:\n",
        "                    pub_list[pub] = {}\n",
        "            if not line.find('#index'):\n",
        "                paper_id = int(line[6:])\n",
        "            if not line.find('#%'):\n",
        "                _cite_temp.append(int(line[2:]))\n",
        "            if line == \"\\n\":\n",
        "                id_pub[paper_id] = pub\n",
        "                id_cite[paper_id] = _cite_temp\n",
        "                _cite_temp = []\n",
        "    return id_pub, id_cite, pub_list \n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYM13IYCWhr6"
      },
      "source": [
        "def build_conference_citation_net(id_pub, id_cite, pub_list):\n",
        "#     build conference citation network\n",
        "#     return list of (conf1, conf2, weight) triples\n",
        "    \n",
        "    for key in id_pub:\n",
        "        pub = id_pub[key]\n",
        "        all_cite = id_cite[key]\n",
        "        for cite in all_cite:\n",
        "            _key = id_pub[cite]\n",
        "            if _key in pub_list[pub]:\n",
        "                pub_list[pub][_key] += 1\n",
        "            else:\n",
        "                pub_list[pub][_key] = 1   \n",
        "    pub_encode = {}\n",
        "    ind = 0\n",
        "    pub_name = []\n",
        "    for key in pub_list:\n",
        "        pub_encode[key] = ind\n",
        "        pub_name.append(key)\n",
        "        ind += 1   \n",
        "    \n",
        "    pub_row, pub_col, value = [],[],[]\n",
        "\n",
        "    for key1 in pub_list:\n",
        "        for key2 in pub_list[key1]:\n",
        "            pub_row.append(pub_encode[key1])\n",
        "            pub_col.append(pub_encode[key2])\n",
        "            value.append(pub_list[key1][key2])\n",
        "    return zip(pub_row,pub_col,value), pub_encode, pub_name"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7LiYHSDWhr7"
      },
      "source": [
        "def normalize(matrix):\n",
        "#     row normalization\n",
        "    rowsum = np.array(matrix.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(matrix)\n",
        "    return mx"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDqBoEhCWhr7"
      },
      "source": [
        "def pagerank(adj, beta):\n",
        "    \"\"\"\n",
        "        TO DO: \n",
        "            compute pagerank scores and return them in numpy array form\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    n, _ = adj.shape\n",
        "    r = np.asarray(adj.sum(axis=1)).reshape(-1)\n",
        "    k = r.nonzero()[0]\n",
        "    D = sp.csr_matrix((1 / r[k], (k, k)), shape=(n, n))\n",
        "    #if personalize is NONE\n",
        "    s = np.ones(n).reshape(n,1)/n\n",
        "    I = np.eye(n)\n",
        "    ranks = sp.linalg.spsolve((I - beta * adj.T @ D), s)\n",
        "\n",
        "    ranks = ranks/ranks.sum()\n",
        "    return ranks\n",
        "    #TODO \n",
        "    "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1esQSBhPWhr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37575752-ad4a-4912-cea0-9c8e366124d1"
      },
      "source": [
        "# Apply your pagerank to the citation network\n",
        "\n",
        "id_pub, id_cite, pub_list = preprocessing()\n",
        "network, pub_encode, pub_name = build_conference_citation_net(id_pub, id_cite, pub_list)\n",
        "\n",
        "col, row, value = zip(*network)\n",
        "adj = coo_matrix((np.array(value), (np.array(row), np.array(col))), dtype = float, shape=(len(pub_list),len(pub_list)))\n",
        "\n",
        "beta = 0.8\n",
        "scores = pagerank(adj, beta)\n",
        "ind = np.argsort(scores)\n",
        "print ('top 50 conferences')\n",
        "rank = 1\n",
        "for i in ind[-50:]:\n",
        "    print (rank, ': ', pub_name[i])\n",
        "    rank = rank + 1\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py:138: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
            "  SparseEfficiencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top 50 conferences\n",
            "1 :  PLDI\n",
            "2 :  Commun. ACM\n",
            "3 :  POPL\n",
            "4 :  PODS\n",
            "5 :  IJIIDS\n",
            "6 :  ACM Trans. Program. Lang. Syst.\n",
            "7 :  ACM Trans. Graph.\n",
            "8 :  Artif. Intell.\n",
            "9 :  DATE\n",
            "10 :  ICCAD\n",
            "11 :  Multimedia Tools Appl.\n",
            "12 :  ER\n",
            "13 :  SIGIR\n",
            "14 :  EICS\n",
            "15 :  WWW\n",
            "16 :  Decision Support Systems\n",
            "17 :  ACM Multimedia\n",
            "18 :  IEEE Trans. Computers\n",
            "19 :  DAC\n",
            "20 :  Fuzzy Sets and Systems\n",
            "21 :  IEEE Trans. Pattern Anal. Mach. Intell.\n",
            "22 :  Future Generation Comp. Syst.\n",
            "23 :  Pattern Recognition Letters\n",
            "24 :  ACM Comput. Surv.\n",
            "25 :  IEEE Trans. Parallel Distrib. Syst.\n",
            "26 :  IJCAI\n",
            "27 :  Information & Software Technology\n",
            "28 :  CHI\n",
            "29 :  Neurocomputing\n",
            "30 :  ICSE\n",
            "31 :  IEEE Trans. Software Eng.\n",
            "32 :  CIKM\n",
            "33 :  Fundam. Inform.\n",
            "34 :  GECCO\n",
            "35 :  Computer Networks\n",
            "36 :  Journal of Systems and Software\n",
            "37 :  VLDB\n",
            "38 :  Data Knowl. Eng.\n",
            "39 :  CoRR\n",
            "40 :  SIGMOD Conference\n",
            "41 :  Computer Communications\n",
            "42 :  Winter Simulation Conference\n",
            "43 :  Pattern Recognition\n",
            "44 :  IEEE Trans. Knowl. Data Eng.\n",
            "45 :  SAC\n",
            "46 :  ICSE (2)\n",
            "47 :  Electr. Notes Theor. Comput. Sci.\n",
            "48 :  Theor. Comput. Sci.\n",
            "49 :  Inf. Sci.\n",
            "50 :  Expert Syst. Appl.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftoELltPWhr7"
      },
      "source": [
        "### Part 4: Personalized-PageRank (20 pts = 5 + 15)\n",
        "\n",
        "For Personalized-PageRank, it is natural to extend queries from single node to a set of nodes. \n",
        "\n",
        "1. Please write down the iterative formula for computing P-PageRank when the query is a set of nodes, and explain why it is designed in the proposed way.\n",
        "2. Please implement the Personalized-PageRank, and show the top-10 most similar conferences to {KDD}, {ICML}, and {KDD, ICML} on the conference citation network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MduBGMoPWhr7"
      },
      "source": [
        "#### Write Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZgGlFVfWhr7"
      },
      "source": [
        "[Your Answer]  https://drive.google.com/file/d/1fh1Q3fV3aqmcAs8mFhtg6KI_44rStOt9/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Y89RkbWhr8"
      },
      "source": [
        "def person_pagerank(adj, beta, target_set):\n",
        "    \"\"\"\n",
        "    To DO:\n",
        "        compute personalized-pagerank scores and return them in numpy array form\n",
        "    \"\"\"\n",
        "    n, _ = adj.shape\n",
        "    r = np.asarray(adj.sum(axis=1)).reshape(-1)\n",
        "    k = r.nonzero()[0]\n",
        "    D = sp.csr_matrix((1 / r[k], (k, k)), shape=(n, n))\n",
        "    \n",
        "    personalize = np.zeros(n)\n",
        "    personalize[target_set] = 1/len(target_set)\n",
        "    personalize = personalize.reshape(n, 1)\n",
        "\n",
        "    I = np.eye(n)\n",
        "    ranks = sp.linalg.spsolve((I - beta * adj.T @ D), personalize)\n",
        "    ranks /= ranks.sum()\n",
        "    return ranks\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwDASFbcWhr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab03f49a-4baa-43d1-ac4f-6a9e2587c7fc"
      },
      "source": [
        "set_KDD = [pub_encode['KDD']]\n",
        "set_ICML = [pub_encode['ICML']]\n",
        "set_both = [pub_encode['KDD'],pub_encode['ICML']]\n",
        "\n",
        "# apply the personalized-pagerank to find top-10 related conferences for KDD\n",
        "beta = 0.8\n",
        "scores = person_pagerank(adj, beta, set_KDD)\n",
        "ind = np.argsort(scores)\n",
        "print ('top 10 conferences for KDD')\n",
        "rank = 1\n",
        "for i in ind[-10:]:\n",
        "    print (rank, ': ', pub_name[i])\n",
        "    rank += 1\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py:138: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
            "  SparseEfficiencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top 10 conferences for KDD\n",
            "1 :  SAC\n",
            "2 :  WWW\n",
            "3 :  Data Knowl. Eng.\n",
            "4 :  VLDB\n",
            "5 :  SIGMOD Conference\n",
            "6 :  IEEE Trans. Knowl. Data Eng.\n",
            "7 :  Inf. Sci.\n",
            "8 :  CIKM\n",
            "9 :  Expert Syst. Appl.\n",
            "10 :  KDD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIIfMg_mWhr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799a533c-0d71-4a4b-8951-52be48525e01"
      },
      "source": [
        "# apply the personalized-pagerank to find top-10 related conferences for ICML\n",
        "beta = 0.8\n",
        "scores = person_pagerank(adj, beta, set_ICML)\n",
        "ind = np.argsort(scores)\n",
        "print ('top 10 conferences for ICML')\n",
        "rank = 1\n",
        "for i in ind[-10:]:\n",
        "    print (rank, ': ', pub_name[i])\n",
        "    rank += 1\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py:138: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
            "  SparseEfficiencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top 10 conferences for ICML\n",
            "1 :  AAAI\n",
            "2 :  Machine Learning\n",
            "3 :  SIGIR\n",
            "4 :  IJCAI\n",
            "5 :  Pattern Recognition\n",
            "6 :  CIKM\n",
            "7 :  KDD\n",
            "8 :  Inf. Sci.\n",
            "9 :  Expert Syst. Appl.\n",
            "10 :  ICML\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_B5eUYiWhr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e0bb73-9d1e-4830-a92e-b3c8db7a2f4a"
      },
      "source": [
        "# apply the personalized-pagerank to find top-10 related conferences for KDD and ICML\n",
        "beta = 0.8\n",
        "scores = person_pagerank(adj, beta, set_both)\n",
        "ind = np.argsort(scores)\n",
        "print ('top 10 conferences for KDD and ICML')\n",
        "rank = 1\n",
        "for i in ind[-10:]:\n",
        "    print (rank, ': ', pub_name[i])\n",
        "    rank += 1"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/linalg/dsolve/linsolve.py:138: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
            "  SparseEfficiencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top 10 conferences for KDD and ICML\n",
            "1 :  SIGMOD Conference\n",
            "2 :  IJCAI\n",
            "3 :  SIGIR\n",
            "4 :  Pattern Recognition\n",
            "5 :  IEEE Trans. Knowl. Data Eng.\n",
            "6 :  CIKM\n",
            "7 :  Inf. Sci.\n",
            "8 :  Expert Syst. Appl.\n",
            "9 :  ICML\n",
            "10 :  KDD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UPNO8-SY--L"
      },
      "source": [
        "# **Citation:**\n",
        "\n",
        "Skip-gram\n",
        "https://stats.stackexchange.com/questions/404878/nlp-how-do-you-randomly-draw-negative-samples\n",
        "\n",
        "https://github.com/blackredscarf/pytorch-SkipGram\n",
        "\n",
        "https://github.com/fanglanting/skip-gram-pytorch\n",
        "\n",
        "\n",
        "P-pagerank: https://web.stanford.edu/class/cs315b/assignment3.html \n",
        "https://asajadi.github.io/fast-pagerank/\n",
        "\n",
        "\n",
        "\n",
        "Classmates that helped each other out with this HW: Yuntian Wang, Jessica Bojorquez, Kevan Loo \n",
        "\n",
        "Gave Raja Vyshnavi Sriramoju, John Fritsche some tips but didn't have in depth conversations"
      ]
    }
  ]
}